{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpamDataset_Gaussian_Naive_Bayes_Classifier\n",
    "Author: Xin Zhengfang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'Xtrain', 'Xtest', 'ytrain', 'ytest'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load .mat file into numpy array\n",
    "mat_contents = sio.loadmat('spamData.mat')\n",
    "mat_contents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3065, 57) (1536, 57) (3065, 1) (1536, 1)\n"
     ]
    }
   ],
   "source": [
    "#  Convert to arrary\n",
    "Xtrain = mat_contents['Xtrain']\n",
    "Xtest = mat_contents['Xtest']\n",
    "ytrain = mat_contents['ytrain']\n",
    "ytest = mat_contents['ytest']\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# Log-transform\n",
    "log_Xtrain = np.log(Xtrain+1e-7)\n",
    "log_Xtest = np.log(Xtest+1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "**The class label:**\n",
    "1. Because dataset has a lot of spam and non-spam emails, we don't need do some prior assumption.The maxmum likelihood $\\lambda^{ML}$ can be used as the plug-in estimator for testing.\n",
    "\n",
    "**The features distribution:**\n",
    "1. To simplify the question, Maxmum likehood is used with univariate gaussian prior.\n",
    "\n",
    "**ML estimation of $\\mu$ ,$\\sigma$ giving training data $D=\\left\\{x_{1}, \\ldots, x_{N}\\right\\}$ $D=\\left\\{x_{1}, \\ldots, x_{N}\\right\\}$:**\n",
    "$$\n",
    "\\begin{aligned} \\frac{\\partial L}{\\partial \\mu} &=\\frac{\\partial}{\\partial \\mu}\\left(\\sum_{n=1}^{N}-\\frac{\\left(x_{n}-\\mu\\right)^{2}}{2 \\sigma^{2}}\\right)=\\sum_{n=1}^{N} \\frac{\\left(x_{n}-\\mu\\right)}{\\sigma^{2}}=0 \\\\ & \\Longrightarrow \\hat{\\mu}=\\frac{1}{N} \\sum_{n=1}^{N} x_{n} \\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned} \\frac{\\partial L}{\\partial \\sigma} &=\\frac{\\partial}{\\partial \\sigma}\\left(\\sum_{n=1}^{N}-\\frac{\\left(x_{n}-\\mu\\right)^{2}}{2 \\sigma^{2}}-N \\log \\sigma\\right)=\\sum_{n} \\frac{\\left(x_{n}-\\mu\\right)^{2}}{\\sigma^{3}}-\\frac{N}{\\sigma}=0 \\\\ & \\Longrightarrow \\hat{\\sigma}^{2}=\\frac{1}{N} \\sum_{n=1}^{N}\\left(x_{n}-\\mu\\right)^{2}=\\frac{1}{N} \\sum_{n=1}^{N}\\left(x_{n}-\\hat{\\mu}\\right)^{2} \\end{aligned}\n",
    "$$\n",
    "Note: See the detailed derivations in Machine_Learning_AXIN_Probabilistic_Perspec(KM)-CHAPTER 4.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainning â˜…\n",
    "'''\n",
    "    To get lambda_ML, mu_jc_ML, sigma_jc_ML lists\n",
    "'''\n",
    "mu_jc_ML = [[],[]]\n",
    "sigma2_jc_ML = [[],[]]\n",
    "num_features = Xtrain.shape[-1]\n",
    "\n",
    "# Pr(y = 1 | lambda_ML)\n",
    "lambda_ML = np.sum(ytrain)/np.sum(np.ones(ytrain.shape))\n",
    "\n",
    "c1_mask = ytrain.repeat(num_features,-1)\n",
    "c0_mask = 1 - c1_mask\n",
    "c1_log_train = log_Xtrain*c1_mask\n",
    "c0_log_train = log_Xtrain*c0_mask\n",
    "\n",
    "for j in range(num_features):\n",
    "    # Pr(x_j0 | y = 0, mu_j0_ML, sigma_j0_ML) = N(xx_j0|mu.sigma^2)\n",
    "    mu_jc_ML[0].append(np.sum(c0_log_train[:,j])/np.sum(1-ytrain))\n",
    "    sigma2_jc_ML[0].append(np.power(np.sum(c0_log_train[:,j]-mu_jc_ML[0][j]),2)/np.sum(1-ytrain))\n",
    "    # Pr(x_j1 | y = 1, mu_j1_ML, sigma_j1_ML) = N(x_j1|mu.sigma^2)\n",
    "    mu_jc_ML[1].append(np.sum(c1_log_train[:,j])/np.sum(ytrain))\n",
    "    sigma2_jc_ML[1].append(np.power(np.sum(c1_log_train[:,j]-mu_jc_ML[1][j]),2)/np.sum(ytrain))\n",
    "mu_jc_ML = np.array(mu_jc_ML)\n",
    "sigma2_jc_ML = np.array(sigma2_jc_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "\n",
    "def UG_pred(log_features,lam,mu,sigma):\n",
    "    '''\n",
    "        Input: \n",
    "            log_zffeatures #log_features of 1 sample\n",
    "            lam,mu_jc_ML,sigma_jc_ML #params of Univarate Gaussian model\n",
    "        Output:\n",
    "            pred #predicted label\n",
    "    '''\n",
    "    pr_c0 = np.prod(1/np.sqrt(2*np.pi*sigma2_jc_ML[0])*np.exp(-0.5*(log_features-mu_jc_ML[0])/sigma2_jc_ML[0]))\n",
    "    pr_c1 = np.prod(1/np.sqrt(2*np.pi*sigma2_jc_ML[1])*np.exp(-0.5*(log_features-mu_jc_ML[1])/sigma2_jc_ML[1]))\n",
    "    if pr_c0 > pr_c1:\n",
    "        pred = 0\n",
    "    else:\n",
    "        pred = 1\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain_pred \n",
    "Xtrain_pred = []\n",
    "for spl in log_Xtrain:\n",
    "    Xtrain_pred.append(UG_pred(spl,lambda_ML,mu_jc_ML,sigma2_jc_ML))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_pred = np.array(Xtrain_pred).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_err = 1 - np.sum((Xtrain_pred == ytrain).astype('int'))/np.array(Xtrain_pred).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtest_pred\n",
    "Xtest_pred = []\n",
    "for spl in log_Xtest:\n",
    "    Xtest_pred.append(UG_pred(spl,lambda_ML,mu_jc_ML,sigma2_jc_ML))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_pred = np.array(Xtest_pred).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_err = 1 - np.sum((Xtest_pred == ytest).astype('int'))/np.array(Xtest_pred).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing error rates for the log-transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error rates:  0.4045676998368679\n",
      "Testing error rates:  0.373046875\n"
     ]
    }
   ],
   "source": [
    "print(\"Training error rates: \",Xtrain_err)\n",
    "print(\"Testing error rates: \",Xtest_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not good. Maybe the Gaussian distribution doesn't fit spamdataset well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
